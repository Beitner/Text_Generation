{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Beitner/Text_Generation/blob/main/Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88X5OhoAJEJh"
      },
      "source": [
        "# RNN for text generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh_aPOY6JEJz"
      },
      "source": [
        "In this notebook, we'll unleash the hidden creativity of a computer, by letting it generate Country songs. we'll train a character-level RNN-based language model, and use it to generate new songs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npjwCeQRJEJ_"
      },
      "source": [
        "## RNN for Text Generation\n",
        "In this section, we'll use an LSTM to generate new songs. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0RjfaAf2N9dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cCmhiSQ2AcIh",
        "outputId": "674e214c-17fa-4fb0-f6b7-c407f8bbf327",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "iNEIbRDo-NoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(r'/content/drive/MyDrive/YANDEX/DeepLearning/NLP/HW1_NLPIntro/metrolyrics.parquet')\n",
        "# selected_text = df[df.genre == 'Country'].lyrics.values"
      ],
      "metadata": {
        "id": "LxvWMhMfbdkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.genre.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTljLg7eFmLm",
        "outputId": "acb6b3b1-940b-4a3f-8f9a-978d9006b475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Pop', 'Hip-Hop', 'Rock', 'Country', 'Metal'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text and Reference Preperation"
      ],
      "metadata": {
        "id": "gpqJuUuuAEzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"lyrics\"] = df['lyrics'].str.lower()\n",
        "\n",
        "df['lyrics'] = df['lyrics'].str.strip('[]')\n",
        "df['lyrics'] = df['lyrics'].str.strip('()')\n",
        "df[\"lyrics\"] = df['lyrics'].str.replace('[^\\w\\s]','')\n",
        "df[\"lyrics\"] = df['lyrics'].str.replace('chorus','')\n",
        "df[\"lyrics\"] = df['lyrics'].str.replace(':','')\n",
        "df[\"lyrics\"] = df['lyrics'].str.replace(',','')\n",
        "df[\"lyrics\"] = df['lyrics'].str.replace('verse','')\n",
        "df[\"lyrics\"] = df['lyrics'].str.replace('x1','')\n",
        "df[\"lyrics\"] = df['lyrics'].str.replace('x2','')\n",
        "df[\"lyrics\"] = df['lyrics'].str.replace('x3','')\n",
        "df[\"lyrics\"] = df['lyrics'].str.replace('x4','')\n",
        "df[\"lyrics\"] = df['lyrics'].str.replace('x5','')\n",
        "df[\"lyrics\"] = df['lyrics'].str.replace('x6','')\n",
        "df[\"lyrics\"] = df['lyrics'].str.replace('x7','')\n",
        "df[\"lyrics\"] = df['lyrics'].str.replace('x8','')\n",
        "df[\"lyrics\"] = df['lyrics'].str.replace('x9','')\n",
        "df[\"lyrics\"] = df['lyrics'].str.encode('ascii', 'ignore').str.decode('ascii')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzT0O4gfkEAe",
        "outputId": "c2665914-c796-4169-e8dc-bbf791934f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "f7YNhRCUk624",
        "outputId": "fb815698-ccac-4ab8-c8a0-699f54cbe0fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     song  year           artist    genre  \\\n",
              "204182                      fully-dressed  2008            annie      Pop   \n",
              "6116                   surrounded-by-hoes  2006          50-cent  Hip-Hop   \n",
              "166369  taste-the-tears-thunderpuss-remix  2006            amber      Pop   \n",
              "198416         the-truth-will-set-me-free  2006     glenn-hughes     Rock   \n",
              "127800                   the-last-goodbye  2008  aaron-pritchett  Country   \n",
              "...                                   ...   ...              ...      ...   \n",
              "33205             give-it-all-up-for-love  2007       bananarama      Pop   \n",
              "194149      all-i-m-thinking-about-is-you  2000  billy-ray-cyrus     Rock   \n",
              "11649                   bonsoir-mon-amour  2015           dalida      Pop   \n",
              "252283             i-m-not-gonna-miss-you  2014    glen-campbell      Pop   \n",
              "11180                        i-am-the-man  2008        the-czars     Rock   \n",
              "\n",
              "                                                   lyrics  num_chars  \\\n",
              "204182  healy\\nspoken this is bert healy saying \\nsing...       1041   \n",
              "6116     repeat 2x even when im tryin to be on the low...       1392   \n",
              "166369  how could you cause me so much pain\\nand leave...       1113   \n",
              "198416  in a scarlet vision\\nin a velvet room\\ni come ...        779   \n",
              "127800  sprintime in savannah\\nit dont get much pretti...        881   \n",
              "...                                                   ...        ...   \n",
              "33205   to all the men i knew before\\nold love letters...       1159   \n",
              "194149  well its a twentyfive mile drive from here to ...       1094   \n",
              "11649   tu viens de partir pour de longs mois cest lon...        455   \n",
              "252283  im still here but yet im gone\\ni dont play gui...        527   \n",
              "11180   you are beyond any reproach now\\nyou are so co...        715   \n",
              "\n",
              "                                                     sent  num_words  \n",
              "204182  healy spoken this bert healy saying singing he...        826  \n",
              "6116    chorus repeat x even i tryin low i recognized ...        884  \n",
              "166369  how could cause much pain and leave heart rain...        756  \n",
              "198416  in scarlet vision in velvet room i come decisi...        583  \n",
              "127800  sprintime savannah it dont get much prettier b...        639  \n",
              "...                                                   ...        ...  \n",
              "33205   to men i knew old love letters drawer mean not...        712  \n",
              "194149  well twenty five mile drive town ther gray ski...        676  \n",
              "11649   tu viens de partir pour de longs mois c est lo...        426  \n",
              "252283  i still yet i gone i play guitar sing songs th...        344  \n",
              "11180   you beyond reproach you cool youre flawless pe...        446  \n",
              "\n",
              "[49976 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fb50f9e7-ca9b-4235-b54d-ffe5f4d4dc54\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>song</th>\n",
              "      <th>year</th>\n",
              "      <th>artist</th>\n",
              "      <th>genre</th>\n",
              "      <th>lyrics</th>\n",
              "      <th>num_chars</th>\n",
              "      <th>sent</th>\n",
              "      <th>num_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>204182</th>\n",
              "      <td>fully-dressed</td>\n",
              "      <td>2008</td>\n",
              "      <td>annie</td>\n",
              "      <td>Pop</td>\n",
              "      <td>healy\\nspoken this is bert healy saying \\nsing...</td>\n",
              "      <td>1041</td>\n",
              "      <td>healy spoken this bert healy saying singing he...</td>\n",
              "      <td>826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6116</th>\n",
              "      <td>surrounded-by-hoes</td>\n",
              "      <td>2006</td>\n",
              "      <td>50-cent</td>\n",
              "      <td>Hip-Hop</td>\n",
              "      <td>repeat 2x even when im tryin to be on the low...</td>\n",
              "      <td>1392</td>\n",
              "      <td>chorus repeat x even i tryin low i recognized ...</td>\n",
              "      <td>884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166369</th>\n",
              "      <td>taste-the-tears-thunderpuss-remix</td>\n",
              "      <td>2006</td>\n",
              "      <td>amber</td>\n",
              "      <td>Pop</td>\n",
              "      <td>how could you cause me so much pain\\nand leave...</td>\n",
              "      <td>1113</td>\n",
              "      <td>how could cause much pain and leave heart rain...</td>\n",
              "      <td>756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198416</th>\n",
              "      <td>the-truth-will-set-me-free</td>\n",
              "      <td>2006</td>\n",
              "      <td>glenn-hughes</td>\n",
              "      <td>Rock</td>\n",
              "      <td>in a scarlet vision\\nin a velvet room\\ni come ...</td>\n",
              "      <td>779</td>\n",
              "      <td>in scarlet vision in velvet room i come decisi...</td>\n",
              "      <td>583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127800</th>\n",
              "      <td>the-last-goodbye</td>\n",
              "      <td>2008</td>\n",
              "      <td>aaron-pritchett</td>\n",
              "      <td>Country</td>\n",
              "      <td>sprintime in savannah\\nit dont get much pretti...</td>\n",
              "      <td>881</td>\n",
              "      <td>sprintime savannah it dont get much prettier b...</td>\n",
              "      <td>639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33205</th>\n",
              "      <td>give-it-all-up-for-love</td>\n",
              "      <td>2007</td>\n",
              "      <td>bananarama</td>\n",
              "      <td>Pop</td>\n",
              "      <td>to all the men i knew before\\nold love letters...</td>\n",
              "      <td>1159</td>\n",
              "      <td>to men i knew old love letters drawer mean not...</td>\n",
              "      <td>712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194149</th>\n",
              "      <td>all-i-m-thinking-about-is-you</td>\n",
              "      <td>2000</td>\n",
              "      <td>billy-ray-cyrus</td>\n",
              "      <td>Rock</td>\n",
              "      <td>well its a twentyfive mile drive from here to ...</td>\n",
              "      <td>1094</td>\n",
              "      <td>well twenty five mile drive town ther gray ski...</td>\n",
              "      <td>676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11649</th>\n",
              "      <td>bonsoir-mon-amour</td>\n",
              "      <td>2015</td>\n",
              "      <td>dalida</td>\n",
              "      <td>Pop</td>\n",
              "      <td>tu viens de partir pour de longs mois cest lon...</td>\n",
              "      <td>455</td>\n",
              "      <td>tu viens de partir pour de longs mois c est lo...</td>\n",
              "      <td>426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252283</th>\n",
              "      <td>i-m-not-gonna-miss-you</td>\n",
              "      <td>2014</td>\n",
              "      <td>glen-campbell</td>\n",
              "      <td>Pop</td>\n",
              "      <td>im still here but yet im gone\\ni dont play gui...</td>\n",
              "      <td>527</td>\n",
              "      <td>i still yet i gone i play guitar sing songs th...</td>\n",
              "      <td>344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11180</th>\n",
              "      <td>i-am-the-man</td>\n",
              "      <td>2008</td>\n",
              "      <td>the-czars</td>\n",
              "      <td>Rock</td>\n",
              "      <td>you are beyond any reproach now\\nyou are so co...</td>\n",
              "      <td>715</td>\n",
              "      <td>you beyond reproach you cool youre flawless pe...</td>\n",
              "      <td>446</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>49976 rows Ã— 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb50f9e7-ca9b-4235-b54d-ffe5f4d4dc54')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fb50f9e7-ca9b-4235-b54d-ffe5f4d4dc54 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fb50f9e7-ca9b-4235-b54d-ffe5f4d4dc54');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the following cell to reduce the size of the text, if the GPU can't handle the training."
      ],
      "metadata": {
        "id": "AGvbQxbUAKFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_text = df[df['genre'] == 'Rock']\n",
        "selected_text = selected_text.lyrics.values"
      ],
      "metadata": {
        "id": "V5fqNuatkI5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dump, small_text_data = train_test_split(selected_text, random_state = 1, test_size = 0.99)"
      ],
      "metadata": {
        "id": "ryrzwgGS5RUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics_data = [y for x in small_text_data for y in x]"
      ],
      "metadata": {
        "id": "lpEZoDme_sQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(small_text_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dhu9c97I_jnQ",
        "outputId": "ee7a72ef-b47c-4bae-8e3f-9c620b393236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11616"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sparse Dataset into letters, and create Mappings"
      ],
      "metadata": {
        "id": "xQB4X1lz8kPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [y for x in lyrics_data for y in x]\n",
        "char_set = set(text)"
      ],
      "metadata": {
        "id": "28nFl2XKcO-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eLXes4Pl8jg",
        "outputId": "659fc62d-d179-491f-d70e-19c331b61be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10978321"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Mappings"
      ],
      "metadata": {
        "id": "CrUpVoz3GHLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_sorted = sorted(char_set)\n",
        "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
        "char_array = np.array(chars_sorted)\n",
        "\n",
        "text_encoded = np.array(\n",
        "    [char2int[ch] for ch in text],\n",
        "    dtype=np.int32)\n",
        "\n",
        "print('Text encoded shape: ', text_encoded.shape)\n",
        "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
        "print('\\n')\n",
        "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(char_array[text_encoded[15:21]]))"
      ],
      "metadata": {
        "id": "iPnLZ6_teMTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91030546-df68-4f46-cc8b-13eb988e97c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text encoded shape:  (10978321,)\n",
            "['i', ' ', 'k', 'n', 'o', 'w', ' ', 'a', ' ', 'c', 'a', 'r', 'p', 'e', 'n']      == Encoding ==>  [22  2 24 27 28 36  2 14  2 16 14 31 29 18 27]\n",
            "\n",
            "\n",
            "[33 18 31  2 36 21]  == Reverse  ==>  ter wh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split texts into chunk sizes for training and prediction."
      ],
      "metadata": {
        "id": "Y-3uXQ2tGOTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 50\n",
        "chunk_size = seq_length + 1 #Each chunk is seq_length +1. In the Dataset, we take the label as the last letter, and the input as all letters before it\n",
        "\n",
        "text_chunks = [text_encoded[i:i+chunk_size] #Create a single chunk as a list, for each chunk available to make.\n",
        "               for i in range(len(text_encoded)-chunk_size+1)] \n",
        "\n",
        "## inspection:\n",
        "for seq in text_chunks[:1]:\n",
        "    input_seq = seq[:seq_length]\n",
        "    target = seq[seq_length] \n",
        "    print(input_seq, ' -> ', target)\n",
        "    print(repr(''.join(char_array[input_seq])), \n",
        "          ' -> ', repr(''.join(char_array[target])))"
      ],
      "metadata": {
        "id": "G1N7MItXf3Yr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e35671f-a833-44fa-c985-fe47bc39d77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[22  2 24 27 28 36  2 14  2 16 14 31 29 18 27 33 18 31  2 36 21 28  2 21\n",
            " 14 17  2 14  2 17 31 18 14 26  1 24 22 25 25 18 17  2 33 21 18  2 26 14\n",
            " 27  2]  ->  15\n",
            "'i know a carpenter who had a dream\\nkilled the man '  ->  'b'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Dataset and DataLoaders"
      ],
      "metadata": {
        "id": "NcUpSCa9Gbna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del df, small_text_data\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In2iYxg40Kiy",
        "outputId": "e324f8ad-9c72-4bb9-9793-9a8d88fbb873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "397"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        return torch.tensor(text_chunk[:-1].long()), torch.tensor(text_chunk[1:].long()) #text chunk is size seq_length +1. Last letter is the label.\n",
        "    \n",
        "seq_dataset = TextDataset(torch.tensor(text_chunks))"
      ],
      "metadata": {
        "id": "UwrEbOzXf3Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.set_device(0)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')# device = 'cpu'\n",
        "from torch.utils.data import DataLoader \n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(1)\n",
        "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True) #Drop the last, non full batch."
      ],
      "metadata": {
        "id": "7Q2_4MIKkahS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Model"
      ],
      "metadata": {
        "id": "NGYIYIwfGjqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, num_layers, drop_prob):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
        "                           batch_first=True, num_layers=num_layers,  dropout=drop_prob)\n",
        "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = torch.tensor(self.embedding(x).unsqueeze(1))\n",
        "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        out = self.fc(out).reshape(out.size(0), -1) #flattens the tensor\n",
        "        return out, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(num_layers, batch_size, self.rnn_hidden_size)\n",
        "        cell = torch.zeros(num_layers, batch_size, self.rnn_hidden_size)\n",
        "        return hidden.to(device), cell.to(device)\n",
        "        "
      ],
      "metadata": {
        "id": "oSLadsy3lFM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameters"
      ],
      "metadata": {
        "id": "GgpPrqFlGnPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(char_array)\n",
        "embed_dim = 512\n",
        "rnn_hidden_size = 256\n",
        "num_layers = 2\n",
        "num_epochs = 3000 \n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "drop_prob = 0.2\n"
      ],
      "metadata": {
        "id": "V_d9FHLnlxDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Prediction"
      ],
      "metadata": {
        "id": "3-kfMaoQGp12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import lr_scheduler\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size, num_layers, drop_prob) \n",
        "# model = Model(char_array)\n",
        "model = model.to(device)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "# scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    hidden, cell = model.init_hidden(batch_size)\n",
        "    seq_batch, target_batch = next(iter(seq_dl))\n",
        "    seq_batch = seq_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    for c in range(seq_length):\n",
        "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell) \n",
        "        loss += loss_fn(pred, target_batch[:, c])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    loss = loss.item()/seq_length\n",
        "    if epoch % 10 == 0:\n",
        "      print(f'Epoch {epoch} loss: {loss:.4f}')"
      ],
      "metadata": {
        "id": "WW9-K8afl0UV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b29de7b9-e738-47a5-e3bd-e7a59a7c35b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 3.6973\n",
            "Epoch 10 loss: 3.0600\n",
            "Epoch 20 loss: 2.7697\n",
            "Epoch 30 loss: 2.5731\n",
            "Epoch 40 loss: 2.3871\n",
            "Epoch 50 loss: 2.3802\n",
            "Epoch 60 loss: 2.4137\n",
            "Epoch 70 loss: 2.0850\n",
            "Epoch 80 loss: 2.2583\n",
            "Epoch 90 loss: 2.0907\n",
            "Epoch 100 loss: 2.0446\n",
            "Epoch 110 loss: 2.1849\n",
            "Epoch 120 loss: 2.2020\n",
            "Epoch 130 loss: 2.0144\n",
            "Epoch 140 loss: 2.1534\n",
            "Epoch 150 loss: 1.9271\n",
            "Epoch 160 loss: 2.0273\n",
            "Epoch 170 loss: 2.1423\n",
            "Epoch 180 loss: 2.1431\n",
            "Epoch 190 loss: 2.3875\n",
            "Epoch 200 loss: 1.9641\n",
            "Epoch 210 loss: 2.0063\n",
            "Epoch 220 loss: 2.0386\n",
            "Epoch 230 loss: 2.0288\n",
            "Epoch 240 loss: 1.8256\n",
            "Epoch 250 loss: 2.1129\n",
            "Epoch 260 loss: 1.8152\n",
            "Epoch 270 loss: 1.7784\n",
            "Epoch 280 loss: 1.9005\n",
            "Epoch 290 loss: 1.8259\n",
            "Epoch 300 loss: 1.9332\n",
            "Epoch 310 loss: 1.8370\n",
            "Epoch 320 loss: 1.9518\n",
            "Epoch 330 loss: 1.9239\n",
            "Epoch 340 loss: 1.9729\n",
            "Epoch 350 loss: 1.9346\n",
            "Epoch 360 loss: 1.8457\n",
            "Epoch 370 loss: 1.9233\n",
            "Epoch 380 loss: 1.8566\n",
            "Epoch 390 loss: 1.8122\n",
            "Epoch 400 loss: 1.6432\n",
            "Epoch 410 loss: 1.8513\n",
            "Epoch 420 loss: 1.7429\n",
            "Epoch 430 loss: 1.9034\n",
            "Epoch 440 loss: 1.8065\n",
            "Epoch 450 loss: 1.8634\n",
            "Epoch 460 loss: 1.8395\n",
            "Epoch 470 loss: 1.8985\n",
            "Epoch 480 loss: 1.7381\n",
            "Epoch 490 loss: 1.6429\n",
            "Epoch 500 loss: 2.0748\n",
            "Epoch 510 loss: 1.7577\n",
            "Epoch 520 loss: 2.0675\n",
            "Epoch 530 loss: 1.7893\n",
            "Epoch 540 loss: 2.1494\n",
            "Epoch 550 loss: 2.0291\n",
            "Epoch 560 loss: 1.7222\n",
            "Epoch 570 loss: 1.7233\n",
            "Epoch 580 loss: 1.8186\n",
            "Epoch 590 loss: 1.8262\n",
            "Epoch 600 loss: 1.8780\n",
            "Epoch 610 loss: 1.9878\n",
            "Epoch 620 loss: 1.6455\n",
            "Epoch 630 loss: 1.7738\n",
            "Epoch 640 loss: 1.6322\n",
            "Epoch 650 loss: 1.9252\n",
            "Epoch 660 loss: 1.9104\n",
            "Epoch 670 loss: 2.1791\n",
            "Epoch 680 loss: 1.6137\n",
            "Epoch 690 loss: 2.0329\n",
            "Epoch 700 loss: 1.8565\n",
            "Epoch 710 loss: 1.8509\n",
            "Epoch 720 loss: 1.6456\n",
            "Epoch 730 loss: 1.6786\n",
            "Epoch 740 loss: 1.9668\n",
            "Epoch 750 loss: 1.7361\n",
            "Epoch 760 loss: 1.7665\n",
            "Epoch 770 loss: 1.8695\n",
            "Epoch 780 loss: 2.0175\n",
            "Epoch 790 loss: 1.8635\n",
            "Epoch 800 loss: 1.9641\n",
            "Epoch 810 loss: 1.8178\n",
            "Epoch 820 loss: 1.7589\n",
            "Epoch 830 loss: 1.6876\n",
            "Epoch 840 loss: 2.1751\n",
            "Epoch 850 loss: 1.7930\n",
            "Epoch 860 loss: 1.8575\n",
            "Epoch 870 loss: 2.0449\n",
            "Epoch 880 loss: 1.9188\n",
            "Epoch 890 loss: 1.7201\n",
            "Epoch 900 loss: 1.8332\n",
            "Epoch 910 loss: 1.8525\n",
            "Epoch 920 loss: 1.6379\n",
            "Epoch 930 loss: 1.5576\n",
            "Epoch 940 loss: 1.9635\n",
            "Epoch 950 loss: 1.8680\n",
            "Epoch 960 loss: 1.7858\n",
            "Epoch 970 loss: 1.9577\n",
            "Epoch 980 loss: 1.7240\n",
            "Epoch 990 loss: 1.8024\n",
            "Epoch 1000 loss: 1.7541\n",
            "Epoch 1010 loss: 1.6827\n",
            "Epoch 1020 loss: 1.9177\n",
            "Epoch 1030 loss: 1.8539\n",
            "Epoch 1040 loss: 1.9509\n",
            "Epoch 1050 loss: 1.8106\n",
            "Epoch 1060 loss: 1.6939\n",
            "Epoch 1070 loss: 1.5260\n",
            "Epoch 1080 loss: 1.7179\n",
            "Epoch 1090 loss: 1.6745\n",
            "Epoch 1100 loss: 1.9960\n",
            "Epoch 1110 loss: 1.7453\n",
            "Epoch 1120 loss: 1.6426\n",
            "Epoch 1130 loss: 1.7935\n",
            "Epoch 1140 loss: 1.5696\n",
            "Epoch 1150 loss: 1.7568\n",
            "Epoch 1160 loss: 1.7603\n",
            "Epoch 1170 loss: 1.9706\n",
            "Epoch 1180 loss: 2.0552\n",
            "Epoch 1190 loss: 1.6083\n",
            "Epoch 1200 loss: 1.6059\n",
            "Epoch 1210 loss: 1.9788\n",
            "Epoch 1220 loss: 1.6572\n",
            "Epoch 1230 loss: 1.7084\n",
            "Epoch 1240 loss: 1.7004\n",
            "Epoch 1250 loss: 1.7853\n",
            "Epoch 1260 loss: 1.6348\n",
            "Epoch 1270 loss: 1.7172\n",
            "Epoch 1280 loss: 1.7118\n",
            "Epoch 1290 loss: 1.8669\n",
            "Epoch 1300 loss: 1.9353\n",
            "Epoch 1310 loss: 1.8208\n",
            "Epoch 1320 loss: 1.7288\n",
            "Epoch 1330 loss: 2.0437\n",
            "Epoch 1340 loss: 1.7326\n",
            "Epoch 1350 loss: 1.7138\n",
            "Epoch 1360 loss: 1.9301\n",
            "Epoch 1370 loss: 1.7497\n",
            "Epoch 1380 loss: 1.6776\n",
            "Epoch 1390 loss: 1.7502\n",
            "Epoch 1400 loss: 1.6785\n",
            "Epoch 1410 loss: 1.9990\n",
            "Epoch 1420 loss: 1.7771\n",
            "Epoch 1430 loss: 1.7743\n",
            "Epoch 1440 loss: 1.7279\n",
            "Epoch 1450 loss: 2.1171\n",
            "Epoch 1460 loss: 1.5893\n",
            "Epoch 1470 loss: 1.6471\n",
            "Epoch 1480 loss: 1.8419\n",
            "Epoch 1490 loss: 1.8165\n",
            "Epoch 1500 loss: 1.7834\n",
            "Epoch 1510 loss: 1.9444\n",
            "Epoch 1520 loss: 1.7571\n",
            "Epoch 1530 loss: 1.6808\n",
            "Epoch 1540 loss: 1.8749\n",
            "Epoch 1550 loss: 1.8374\n",
            "Epoch 1560 loss: 1.4583\n",
            "Epoch 1570 loss: 1.8454\n",
            "Epoch 1580 loss: 1.6317\n",
            "Epoch 1590 loss: 2.0039\n",
            "Epoch 1600 loss: 1.6059\n",
            "Epoch 1610 loss: 1.6946\n",
            "Epoch 1620 loss: 1.7138\n",
            "Epoch 1630 loss: 1.6515\n",
            "Epoch 1640 loss: 1.6122\n",
            "Epoch 1650 loss: 2.0911\n",
            "Epoch 1660 loss: 1.6081\n",
            "Epoch 1670 loss: 1.5682\n",
            "Epoch 1680 loss: 1.5781\n",
            "Epoch 1690 loss: 1.8102\n",
            "Epoch 1700 loss: 1.6713\n",
            "Epoch 1710 loss: 1.8054\n",
            "Epoch 1720 loss: 1.6984\n",
            "Epoch 1730 loss: 1.7962\n",
            "Epoch 1740 loss: 1.9705\n",
            "Epoch 1750 loss: 1.7587\n",
            "Epoch 1760 loss: 1.6090\n",
            "Epoch 1770 loss: 1.7919\n",
            "Epoch 1780 loss: 1.5735\n",
            "Epoch 1790 loss: 1.5883\n",
            "Epoch 1800 loss: 1.6163\n",
            "Epoch 1810 loss: 1.7504\n",
            "Epoch 1820 loss: 1.8197\n",
            "Epoch 1830 loss: 1.6573\n",
            "Epoch 1840 loss: 1.6584\n",
            "Epoch 1850 loss: 1.7714\n",
            "Epoch 1860 loss: 1.7149\n",
            "Epoch 1870 loss: 1.6448\n",
            "Epoch 1880 loss: 1.6690\n",
            "Epoch 1890 loss: 1.6497\n",
            "Epoch 1900 loss: 1.8781\n",
            "Epoch 1910 loss: 1.6974\n",
            "Epoch 1920 loss: 1.5507\n",
            "Epoch 1930 loss: 1.9347\n",
            "Epoch 1940 loss: 1.9480\n",
            "Epoch 1950 loss: 1.5355\n",
            "Epoch 1960 loss: 1.5988\n",
            "Epoch 1970 loss: 1.6559\n",
            "Epoch 1980 loss: 1.9866\n",
            "Epoch 1990 loss: 1.6544\n",
            "Epoch 2000 loss: 2.3766\n",
            "Epoch 2010 loss: 1.6381\n",
            "Epoch 2020 loss: 1.8980\n",
            "Epoch 2030 loss: 1.6722\n",
            "Epoch 2040 loss: 1.8921\n",
            "Epoch 2050 loss: 1.7099\n",
            "Epoch 2060 loss: 1.7048\n",
            "Epoch 2070 loss: 1.5774\n",
            "Epoch 2080 loss: 1.5149\n",
            "Epoch 2090 loss: 1.6897\n",
            "Epoch 2100 loss: 1.7319\n",
            "Epoch 2110 loss: 1.4824\n",
            "Epoch 2120 loss: 1.5365\n",
            "Epoch 2130 loss: 1.6819\n",
            "Epoch 2140 loss: 1.5629\n",
            "Epoch 2150 loss: 1.7850\n",
            "Epoch 2160 loss: 1.7284\n",
            "Epoch 2170 loss: 1.8730\n",
            "Epoch 2180 loss: 1.8255\n",
            "Epoch 2190 loss: 1.6011\n",
            "Epoch 2200 loss: 1.6074\n",
            "Epoch 2210 loss: 1.8088\n",
            "Epoch 2220 loss: 1.7641\n",
            "Epoch 2230 loss: 1.7263\n",
            "Epoch 2240 loss: 1.6124\n",
            "Epoch 2250 loss: 1.5911\n",
            "Epoch 2260 loss: 1.4984\n",
            "Epoch 2270 loss: 1.8833\n",
            "Epoch 2280 loss: 1.6926\n",
            "Epoch 2290 loss: 1.7247\n",
            "Epoch 2300 loss: 1.4574\n",
            "Epoch 2310 loss: 1.4932\n",
            "Epoch 2320 loss: 1.7919\n",
            "Epoch 2330 loss: 1.6102\n",
            "Epoch 2340 loss: 1.6351\n",
            "Epoch 2350 loss: 1.7158\n",
            "Epoch 2360 loss: 1.6933\n",
            "Epoch 2370 loss: 1.7481\n",
            "Epoch 2380 loss: 1.5554\n",
            "Epoch 2390 loss: 1.8115\n",
            "Epoch 2400 loss: 1.4960\n",
            "Epoch 2410 loss: 1.5528\n",
            "Epoch 2420 loss: 1.5555\n",
            "Epoch 2430 loss: 1.6462\n",
            "Epoch 2440 loss: 1.5784\n",
            "Epoch 2450 loss: 1.6539\n",
            "Epoch 2460 loss: 1.6582\n",
            "Epoch 2470 loss: 1.7388\n",
            "Epoch 2480 loss: 1.5888\n",
            "Epoch 2490 loss: 1.6636\n",
            "Epoch 2500 loss: 1.5455\n",
            "Epoch 2510 loss: 1.5436\n",
            "Epoch 2520 loss: 1.7153\n",
            "Epoch 2530 loss: 1.8279\n",
            "Epoch 2540 loss: 1.6987\n",
            "Epoch 2550 loss: 1.8392\n",
            "Epoch 2560 loss: 1.6289\n",
            "Epoch 2570 loss: 1.7663\n",
            "Epoch 2580 loss: 1.8602\n",
            "Epoch 2590 loss: 2.1467\n",
            "Epoch 2600 loss: 1.6249\n",
            "Epoch 2610 loss: 1.6163\n",
            "Epoch 2620 loss: 1.6046\n",
            "Epoch 2630 loss: 1.5425\n",
            "Epoch 2640 loss: 1.5173\n",
            "Epoch 2650 loss: 1.6287\n",
            "Epoch 2660 loss: 1.6960\n",
            "Epoch 2670 loss: 1.6172\n",
            "Epoch 2680 loss: 1.5645\n",
            "Epoch 2690 loss: 1.6257\n",
            "Epoch 2700 loss: 1.7273\n",
            "Epoch 2710 loss: 1.5450\n",
            "Epoch 2720 loss: 1.8696\n",
            "Epoch 2730 loss: 1.4513\n",
            "Epoch 2740 loss: 1.5208\n",
            "Epoch 2750 loss: 1.5923\n",
            "Epoch 2760 loss: 1.8022\n",
            "Epoch 2770 loss: 1.7119\n",
            "Epoch 2780 loss: 1.4847\n",
            "Epoch 2790 loss: 1.6615\n",
            "Epoch 2800 loss: 1.6102\n",
            "Epoch 2810 loss: 1.5553\n",
            "Epoch 2820 loss: 1.8209\n",
            "Epoch 2830 loss: 1.7400\n",
            "Epoch 2840 loss: 1.5678\n",
            "Epoch 2850 loss: 1.6748\n",
            "Epoch 2860 loss: 1.7210\n",
            "Epoch 2870 loss: 1.6555\n",
            "Epoch 2880 loss: 1.7377\n",
            "Epoch 2890 loss: 1.6165\n",
            "Epoch 2900 loss: 1.4796\n",
            "Epoch 2910 loss: 1.5554\n",
            "Epoch 2920 loss: 1.5334\n",
            "Epoch 2930 loss: 1.6280\n",
            "Epoch 2940 loss: 1.5639\n",
            "Epoch 2950 loss: 1.5778\n",
            "Epoch 2960 loss: 1.4501\n",
            "Epoch 2970 loss: 1.6611\n",
            "Epoch 2980 loss: 1.7053\n",
            "Epoch 2990 loss: 1.6009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions.categorical import Categorical\n",
        "def sample(model, text, \n",
        "           len_generated_text=500, \n",
        "           scale_factor=1):\n",
        "\n",
        "    encoded_input = torch.tensor([char2int[s] for s in text]).to(device)\n",
        "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
        "\n",
        "    generated_str = text\n",
        "\n",
        "    model.eval()\n",
        "    hidden, cell = model.init_hidden(1)\n",
        "    hidden = hidden.to(device) #.to('cpu')\n",
        "    cell = cell.to(device) #.to('cpu')\n",
        "    for c in range(len(text)-1):\n",
        "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell) \n",
        "    \n",
        "    last_char = encoded_input[:, -1]\n",
        "    for i in range(len_generated_text):\n",
        "        logits, hidden, cell = model(last_char.view(1), hidden, cell) \n",
        "        logits = torch.squeeze(logits, 0)\n",
        "        scaled_logits = logits * scale_factor\n",
        "        m = Categorical(logits=scaled_logits)\n",
        "        last_char = m.sample()\n",
        "        generated_str += str(char_array[last_char])\n",
        "        \n",
        "    return generated_str\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model.to(device)\n",
        "# print(sample(model, 'have a little love on a little honeymoon, you got a little dish and you got a little spoon. a little bitty house and a little bitty yard'))\n",
        "print(sample(model, 'i love you'))"
      ],
      "metadata": {
        "id": "ASfwPWoF4Z4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e98d5aa1-1ef7-41fc-e791-13bfd8596bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i love youre she look in my heart ad day\n",
            "to less just wanna creep pieting of the light you\n",
            "mets gonna be for lets now just and a love to mind\n",
            "to fever go\n",
            "you live collid lonely to be unschoken one go\n",
            "always do out that a youve just till be bited\n",
            "so ivr heek the destill sg for a just we have to a gleme to te desion\n",
            "its please my body get the but i waiting it for youh\n",
            "its waiting we wont hard to tlose to be come you looker womblie beautiful here\n",
            "you more as well it squence away\n",
            "law\n",
            "was a shialonel intellen \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(model,'we are the champions'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ1v-Yd2npy_",
        "outputId": "249ed670-5867-4676-936d-d63b42a7e0ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we are the champions\n",
            "and a retude\n",
            "needlity mey goodbye is weve got to know where a plues mind\n",
            "as the heaving meners\n",
            "du im joken\n",
            "all the body baby know\n",
            "where the matter meetybody sky\n",
            "firso\n",
            "got see home one kir\n",
            "hurda fact to osterd the were to know\n",
            "hide we nothing i wish how i cant be we go for the long\n",
            "somelone everybody to dead long\n",
            "i would preast red twont truth was becore call my cestle one just everywhere some hard\n",
            "when the flew yi swear case lose our breaks at the pety guess something now\n",
            "go do out to were new \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "Copy of H.W_9_Text_Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}